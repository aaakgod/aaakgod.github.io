---
layout: post
title: "Logistic Regression"
subtitle: 'Logistic Regression'
author: "Kgod"
header-style: text
tags:
  - Logistic Regression
---
# Logistic Regression

归一化 （Normalization）、标准化 （Standardization）和中心化/零均值化 （Zero-centered）


1 概念
  归一化：１）把数据变成(０，１)或者（1,1）之间的小数。主要是为了数据处理方便提出来的，把数据映射到0～1范围之内处理，更加便捷快速。２）把有量纲表达式变成无量纲表达式，便于不同单位或量级的指标能够进行比较和加权。归一化是一种简化计算的方式，即将有量纲的表达式，经过变换，化为无量纲的表达式，成为纯量。
  标准化：在机器学习中，我们可能要处理不同种类的资料，例如，音讯和图片上的像素值，这些资料可能是高维度的，资料标准化后会使每个特征中的数值平均变为0(将每个特征的值都减掉原始资料中该特征的平均)、标准差变为1，这个方法被广泛的使用在许多机器学习算法中(例如：支持向量机、逻辑回归和类神经网络)。
  中心化：平均值为0，对标准差无要求
  归一化和标准化的区别：归一化是将样本的特征值转换到同一量纲下把数据映射到[0,1]或者[-1, 1]区间内，仅由变量的极值决定，因区间放缩法是归一化的一种。标准化是依照特征矩阵的列处理数据，其通过求z-score的方法，转换为标准正态分布，和整体样本分布相关，每个样本点都能对标准化产生影响。它们的相同点在于都能取消由于量纲不同引起的误差；都是一种线性变换，都是对向量X按照比例压缩再进行平移。
  标准化和中心化的区别：标准化是原始分数减去平均数然后除以标准差，中心化是原始分数减去平均数。 所以一般流程为先中心化再标准化。
  无量纲：我的理解就是通过某种方法能去掉实际过程中的单位，从而简化计算。

2 为什么要归一化/标准化？
  如前文所说，归一化/标准化实质是一种线性变换，线性变换有很多良好的性质，这些性质决定了对数据改变后不会造成“失效”，反而能提高数据的表现，这些性质是归一化/标准化的前提。比如有一个很重要的性质：线性变换不会改变原始数据的数值排序。
（1）某些模型求解需要
    1）在使用梯度下降的方法求解最优化问题时， 归一化/标准化后可以加快梯度下降的求解速度，即提升模型的收敛速度。如左图所示，未归一化/标准化时形成的等高线偏椭圆，迭代时很有可能走“之”字型路线（垂直长轴），从而导致迭代很多次才能收敛。而如右图对两个特征进行了归一化，对应的等高线就会变圆，在梯度下降进行求解时能较快的收敛。
    2）一些分类器需要计算样本之间的距离(如欧氏距离)，例如KNN。如果一个特征值域范围非常大，那么距离计算就主要取决于这个特征，从而与实际情况相悖(比如这时实际情况是值域范围小的特征更重要)。

（2）无量纲化
  例如房子数量和收入，因为从业务层知道，这两者的重要性一样，所以把它们全部归一化。 这是从业务层面上作的处理。

（3）避免数值问题
  太大的数会引发数值问题。

3 数据预处理时
3.1 归一化
（1）Min-Max Normalization
   x' = (x - X_min) / (X_max - X_min)

（2）平均归一化
   x' = (x - μ) / (MaxValue - MinValue)
  （1）和（2）有一个缺陷就是当有新数据加入时，可能导致max和min的变化，需要重新定义。

（3）非线性归一化
  1）对数函数转换：y = log10(x)
  2）反余切函数转换：y = atan(x) * 2 / π
  （3）经常用在数据分化比较大的场景，有些数值很大，有些很小。通过一些数学函数，将原始值进行映射。该方法包括 log、指数，正切等。需要根据数据分布的情况，决定非线性函数的曲线，比如log(V, 2)还是log(V, 10)等。

3.2 标准化
（1）Z-score规范化（标准差标准化 / 零均值标准化）
  x' = (x - μ)／σ

3.3 中心化
  x' = x - μ

4 什么时候用归一化？什么时候用标准化？
  （1）如果对输出结果范围有要求，用归一化。
  （2）如果数据较为稳定，不存在极端的最大最小值，用归一化。
  （3）如果数据存在异常值和较多噪音，用标准化，可以间接通过中心化避免异常值和极端值的影响。
  某知乎答主的回答提到了他个人经验：一般来说，我个人建议优先使用标准哈。对于输出有要求时再尝试别的方法，如归一化或者更加复杂的方法。很多方法都可以将输出范围调整到[0, 1]，如果我们对于数据的分布有假设的话，更加有效的方法是使用相对应的概率密度函数来转换。让我们以高斯分布为例，我们可以首先计算高斯误差函数（Gaussian Error Function），此处定为er fc(·)，那么可以用下式进行转化：

  这篇博客提到他的经验：1） 在分类、聚类算法中，需要使用距离来度量相似性的时候、或者使用PCA技术进行降维的时候，第二种方法(Z-score standardization)表现更好。2） 在不涉及距离度量、协方差计算、数据不符合正太分布的时候，可以使用第一种方法或其他归一化方法。比如图像处理中，将RGB图像转换为灰度图像后将其值限定在[0 255]的范围。
5 哪些模型必须归一化/标准化？
（1）SVM
  不同的模型对特征的分布假设是不一样的。比如SVM 用高斯核的时候，所有维度共用一个方差，这不就假设特征分布是圆的么，输入椭圆的就坑了人家，所以简单的归一化都还不够好，来杯白化才有劲。比如用树的时候就是各个维度各算各的切分点，没所谓。

（2）KNN
  需要度量距离的模型，一般在特征值差距较大时，都会进行归一化/标准化。不然会出现“大数吃小数”。

（3）神经网络
  1）数值问题
  归一化/标准化可以避免一些不必要的数值问题。输入变量的数量级未致于会引起数值问题吧，但其实要引起也并不是那么困难。因为tansig（tanh）的非线性区间大约在[-1.7，1.7]。意味着要使神经元有效，tansig( w1x1 + w2x2 +b) 里的 w1x1 +w2x2 +b 数量级应该在 1 （1.7所在的数量级）左右。这时输入较大，就意味着权值必须较小，一个较大，一个较小，两者相乘，就引起数值问题了。
  假如你的输入是421，你也许认为，这并不是一个太大的数，但因为有效权值大概会在1/421左右，例如0.00243，那么，在matlab里输入 421·0.00243 == 0.421·2.43，会发现不相等，这就是一个数值问题。

  2）求解需要
  a. 初始化：在初始化时我们希望每个神经元初始化成有效的状态，tansig函数在[-1.7, 1.7]范围内有较好的非线性，所以我们希望函数的输入和神经元的初始化都能在合理的范围内使得每个神经元在初始时是有效的。（如果权值初始化在[-1,1]且输入没有归一化且过大，会使得神经元饱和）
  b. 梯度：以输入-隐层-输出这样的三层BP为例，我们知道对于输入-隐层权值的梯度有2ew(1-a^2)*x的形式（e是誤差，w是隐层到输出层的权重，a是隐层神经元的值，x是输入），若果输出层的数量级很大，会引起e的数量级很大，同理，w为了将隐层（数量级为1）映身到输出层，w也会很大，再加上x也很大的话，从梯度公式可以看出，三者相乘，梯度就非常大了。这时会给梯度的更新带来数值问题。
  c. 学习率：由（2）中，知道梯度非常大，学习率就必须非常小，因此，学习率（学习率初始值）的选择需要参考输入的范围，不如直接将数据归一化，这样学习率就不必再根据数据范围作调整。 隐层到输出层的权值梯度可以写成 2ea，而输入层到隐层的权值梯度为 2ew(1-a^2)x ，受 x 和 w 的影响，各个梯度的数量级不相同，因此，它们需要的学习率数量级也就不相同。对w1适合的学习率，可能相对于w2来说会太小，若果使用适合w1的学习率，会导致在w2方向上步进非常慢，会消耗非常多的时间，而使用适合w2的学习率，对w1来说又太大，搜索不到适合w1的解。如果使用固定学习率，而数据没归一化，则后果可想而知。
  d.搜索轨迹：已解释
  
（4）PCA

参考：
标准化和归一化什么区别？ - 知乎：https://www.zhihu.com/question/20467170
R--数据标准化、归一化、中心化处理：https://zhuanlan.zhihu.com/p/33727799
特征工程中的[归一化]有什么作用？ - 知乎：https://www.zhihu.com/question/20455227
神经网络为什么要归一化：http://nnetinfo.com/nninfo/showText.jsp?id=37

====================
 
均值插补：
对于定类数据：使用 众数（mode）填补，比如一个学校的男生和女生的数量，男生500人，女生50人，那么对于其余的缺失值我们会用人数较多的男生来填补。
对于定量（定比）数据：使用平均数（mean）或中位数（median）填补，比如一个班级学生的身高特征，对于一些同学缺失的身高值就可以使用全班同学身高的平均值或中位数来填补。一般如果特征分布为正太分布时，使用平均值效果比较好，而当分布由于异常值存在而不是正太分布的情况下，使用中位数效果比较好.

注：此方法虽然简单，但是不够精准，可能会引入噪声，或者会改变特征原有的分布。
下图左为填补前的特征分布，图右为填补后的分布，明显发生了畸变。因此，如果缺失值是随机性的，那么用平均值比较适合保证无偏，否则会改变原分布。

一般来说，最终的正确率，训练集大于验证集，验证集大于测试集。  
如果训练集和测试集(验证集)上，正确率都很低，那么，说明模型处于欠拟合状态，需要调整超参数。如果训练集上正确率很低，测试集(或验证集)上正确率较高，说明数据集有问题。如果训练集上正确率很高，测试集(以及验证集)上正确率较低，说明模型过拟合，需要进行正则化或者Dropout来抑制过拟合。如果训练集和验证集上正确率都很高，但是在测试集上正确率较低，那么说明模型的泛化能力不足，调整方法可参考之前的过拟合情况。如果训练集和测试集(包括验证集)上模型的正确率都很高，那么，恭喜你！

=============================

根据数据集中的变量，可将这些变量进行分类，有助于加深对变量的理解。大致可以分为以下几个维度：  

    人口统计信息：借款人年龄、家庭成员数量  

    信用历史纪录：两年内35-59天逾期次数、两年内60-89天逾期次数、两年内90天或高于90天逾期次数  

    偿债能力：负债比率、月收入、贷款数量、不安全额度循环利用、不动产贷款或额度数量  
 
数据清洗过程大致包含：  
    选择子集  
    缺失数据处理  删除  填充  异常值  
    数据类型转换  
    数据排序  
    异常值处理  

左偏和右偏:  
偏态分布为统计学概念，即统计数据峰值与平均值不相等的频率分布。根据峰值小于或大于平均值可分为正偏函数和负偏函数，其偏离的程度可用偏态系数刻画。（苦了我这个大学概率论考了78，高数89的。线性代数98的人）

<img src="/picturesWork/logisticPredict/01.png">

左偏分布（负偏态）中：mean（平均数）< median（中位数）< mode（众数）  

右偏分布（正偏态）中：mode（众数）< median（中位数）< mean（平均数）  

偏度本身是相对于均值左右数据的多少。右偏（尾巴一定是在右边），也就是说数据在均值左侧的数量较多，所以为了达到所有数据于均值之差和为0,应该存在较大的数与之平衡，所有分布图里有一个很长的右端的拖尾（就是右端必须存在很大的值）。既然均值左侧的数比较多，对比中位数左右两侧数一样多，则均值必在中位数的右侧（即这样围成面积才大于0.5)。另外，右偏的图像围成面积为0.5的分界点应该在峰值点的右侧，所以中位数大于众数。所以就有众小于中小于均。

运用到实际中：  
 金融行业中不同类型的风控企业他的偏态分布体现了他的战略体系和风控体系。传统商业银行的信用风险分布是——左偏分布。因为他的经营模式是固定债权人，低杠杆、低风险（利率低，相比较私募股权投资等）），表现出的偏态特征就是：  

    （1）他的大部分数值集中在均值右侧——frequent small gains（频繁小幅收益），  

    （2）同时有一个很长的左端拖尾（就是左端存在很大的值），信用风险（极端值）一旦出现会有非常大的损失——a few extreme losses.
     当然描述性统计属于定量分析，而几乎所有的定量分析都具有滞后性，也就是说我们是根据样本分布找到总体参数，从而分析总体特征，这些特征都是滞后表现的。

 








